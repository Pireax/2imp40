{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe0febb",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a93585-8e41-49de-af2a-c282edfe2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Python libraries\n",
    "!pip install pymongo==3.7.2 folium==0.9.1 geopandas==0.7.0 seaborn numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d2ecb-a37e-40ea-88e8-816a6227b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo                            # Library to access MongoDB\n",
    "from pymongo import MongoClient           # Imports MongoClient \n",
    "import pandas as pd                       # Library to work with dataframes\n",
    "import folium                             # Library to create the map\n",
    "from folium import plugins                # plugin to create a heatmap\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b22e4-2630-4367-9abc-808201672069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uri (uniform resource identifier) defines the connection parameters \n",
    "uri = 'mongodb://127.0.0.1:27017'\n",
    "# start client to connect to MongoDB server \n",
    "client = MongoClient( uri )\n",
    "db = client.JiraRepos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e38cf-fd4d-4216-b120-6dc32e5b6eb4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7829a3-d60e-484f-b45a-d34b08868d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def issue_map():\n",
    "    \"\"\"Get the custom issue mapping as a dictionary.\n",
    "    \"\"\"\n",
    "    return dict(pd.read_csv('issue_map.csv').values)\n",
    "\n",
    "# Cache as global variable here.\n",
    "issue_dict = issue_map()\n",
    "def standardize_issue_name(name):\n",
    "    \"\"\"Try to standardize issue types so they can be shown in less columns.\n",
    "    \"\"\"\n",
    "    name = name.strip()\n",
    "    return issue_dict[name] if name in issue_dict else name\n",
    "\n",
    "def lexicon_to_array():\n",
    "    lexicon = pd.read_csv('Harassment Lexicon.csv')\n",
    "    arr = lexicon.fillna('').astype(str).values.flatten().tolist()\n",
    "    # Ditch some words here with special characters.\n",
    "    return [i for i in arr if i and not re.search(\"[()*!?]\", i)]\n",
    "\n",
    "def lexicon_to_space_delimited():\n",
    "    return \" \".join(lexicon_to_array())\n",
    "\n",
    "def find_all_issue_types():\n",
    "    \"\"\"Find all issue types so we can run multiple queries that may run different subsets, allowing for setting 0's.\n",
    "    \"\"\"\n",
    "    all_issue_types = set()\n",
    "    collections = db.list_collection_names()[:7]\n",
    "    \n",
    "    progress_bar = tqdm(collections)\n",
    "    for name in progress_bar:\n",
    "        all_issue_types.update(set([standardize_issue_name(row[\"_id\"]) for row in query_count_by_issue_type_without_sum(db[name])]))\n",
    "    return all_issue_types\n",
    "\n",
    "def query_count_by_issue_type(collection):\n",
    "    return query_aggregate([\n",
    "        { \"$group\": { \"_id\": '$fields.issuetype.name', \"count\": {\"$sum\": 1} } },\n",
    "    ], collection)\n",
    "\n",
    "def query_count_by_issue_type_without_sum(collection):\n",
    "    return query_aggregate([\n",
    "        { \"$group\": { \"_id\": '$fields.issuetype.name'} },\n",
    "    ], collection)\n",
    "\n",
    "def query_count_by_issue_type_having_comments(collection):\n",
    "     return query_aggregate([\n",
    "        { \"$match\": { \"fields.comments\": { \"$exists\": \"true\", \"$type\": \"array\", \"$ne\": [] } } },\n",
    "        { \"$group\": { \"_id\": '$fields.issuetype.name', \"count\": {\"$sum\": 1} } },\n",
    "    ], collection)\n",
    "\n",
    "def query_count_by_issue_type_having_toxic_comments(collection):\n",
    "    \"\"\" (DONT USE THIS) This solution uses a regex and is very slow (also the format is fucked up due to the projection).\n",
    "    \"\"\"\n",
    "    return query_aggregate([\n",
    "        { \"$project\": {\n",
    "            \"fields.comments\": {\n",
    "                \"$filter\": {\n",
    "                    \"input\": \"$fields.comments.body\",\n",
    "                    \"as\": \"varcommentbody\",\n",
    "                    \"cond\": { \"$regexMatch\": { \"input\": \"$$varcommentbody\", \"regex\": \"/\" + \"|\".join(lexicon_to_array()) + \"/\", \"options\": \"i\" } },\n",
    "                }\n",
    "            }\n",
    "        } },\n",
    "        { \"$match\": { \"fields.comments\": { \"$exists\": \"true\", \"$type\": \"array\", \"$ne\": [] } } },\n",
    "        { \"$group\": { \"_id\": '$fields.issuetype.name', \"count\": {\"$sum\": 1} } },\n",
    "    ], collection)\n",
    "\n",
    "def query_count_by_issue_type_having_toxic_comments_fts(collection):\n",
    "    \"\"\"Uses a full-text search.\n",
    "    \"\"\"\n",
    "    # Create index for specific column. A collection can only have 1 text index and it is automatically the one used.\n",
    "    collection.create_index([('fields.comments.body', pymongo.TEXT)], name='my_search_index', default_language='english')\n",
    "    return query_aggregate([\n",
    "        { \"$match\": { \"$text\": { \"$search\": lexicon_to_space_delimited() } } },\n",
    "        { \"$group\": { \"_id\": '$fields.issuetype.name', \"count\": {\"$sum\": 1} } },\n",
    "    ], collection)\n",
    "\n",
    "def query_get_issues_with_toxic_comments(collection, limit=5):\n",
    "    return list(collection.find({ \"$text\": { \"$search\": lexicon_to_space_delimited() } }).limit(limit))\n",
    "\n",
    "\n",
    "def query_aggregate(query, collection):\n",
    "    \"\"\" Convenience wrapper to track time.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    result = list(collection.aggregate(query))\n",
    "    end = time.time()\n",
    "    # print(\"Duration: {} seconds\".format(end - start))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5b985-49f7-46ba-9901-1cfa53c911d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_comments_per_issue(query_fn_string, issue_types):\n",
    "    \"\"\"This function runs a count query per issue type on each collection in our Jira database. It might take a minute or so.\n",
    "    \n",
    "    TODO: filter comments by toxic words (hopefully i can just edit the mongodb query to accept a list)\n",
    "    \n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    collections = db.list_collection_names()\n",
    "    # collections = np.array(db.list_collection_names())[:2]\n",
    "    \n",
    "    progress_bar = tqdm(collections)\n",
    "    for name in progress_bar:\n",
    "        progress_bar.set_description(\"Processing %s\" % name)\n",
    "        # Call function from string.\n",
    "        result = globals()[query_fn_string](db[name])\n",
    "        subdata = {};\n",
    "        # Add any issue types with count data to the subdata.\n",
    "        for row in result:\n",
    "            issue_type = standardize_issue_name(row['_id'])\n",
    "            if issue_type in subdata:\n",
    "                subdata[issue_type] += row['count']\n",
    "            else:\n",
    "                subdata[issue_type] = row['count']\n",
    "        data.append([name, subdata])\n",
    "            \n",
    "    # Fill in the complete matrix because issue types differ per subset.\n",
    "    data2 = []\n",
    "    for row in data:\n",
    "        # First column = name\n",
    "        subdata = [row[0]]\n",
    "        for issue_type in issue_types:\n",
    "            subdata.append(row[1][issue_type] if issue_type in row[1] else 0)\n",
    "        data2.append(subdata)\n",
    "\n",
    "    data2 = np.array(data2)\n",
    "    # # Use first column as index, rest as the data.\n",
    "    df = pd.DataFrame(np.array(data2[:,1:], dtype=np.int32), columns=list(issue_types), index=data2[:,0])   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxic_comments(collection, limit=5, colorize=True):\n",
    "    def red(text):\n",
    "        return \"\\x1b[31m\" + text + \"\\x1b[0m\"\n",
    "    \n",
    "    lexicon_regex = \"|\".join(['\\\\b' + i + \"\\\\b\" for i in lexicon_to_array()])\n",
    "    results = query_get_issues_with_toxic_comments(collection, limit)\n",
    "    comments = []\n",
    "    for result in results:\n",
    "        for comment in result['fields']['comments']:\n",
    "            found = re.search(lexicon_regex, str(comment), re.IGNORECASE)\n",
    "            if(found):\n",
    "                comments.append(\"[FOUND: \" + red(found.group()) + \"] \" + re.sub(\"\\\\b\" + found.group() + \"\\\\b\", red(found.group()), comment['body']))\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2e714-e16d-48ab-9734-edc453de6c4f",
   "metadata": {},
   "source": [
    "# Execution + Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5541a93-e9fd-4e48-89fa-f371ac659db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_issue_types = find_all_issue_types()\n",
    "print(all_issue_types)\n",
    "df_comments = find_comments_per_issue(\"query_count_by_issue_type_having_comments\", all_issue_types)\n",
    "df_toxic_comments = find_comments_per_issue(\"query_count_by_issue_type_having_toxic_comments_fts\", all_issue_types)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8ee53-c0d9-440f-b0a3-472fbafb4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_comments.shape)\n",
    "print(df_toxic_comments.shape)\n",
    "\n",
    "# Divide toxic comments by total comments to get a percentage.\n",
    "df_div = df_toxic_comments.div(df_comments).fillna(0)\n",
    "\n",
    "display(df_comments)\n",
    "display(df_toxic_comments)\n",
    "display(df_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864c353-387e-4044-9bee-0e710effa00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_heatmap(data, title, xlabel, ylabel, fmt='d'):\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(data, annot=True, square=True, fmt=fmt)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af3e50-d2af-46c6-b794-0111c7a33c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_heatmap(df_div.multiply(100).astype(int).sort_index(axis=0).sort_index(axis=1).T, 'Average % of toxicity', 'Jira dataset', 'Issue type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae3d7c-a5e8-4d83-8c96-6a8a4c5f6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_heatmap(df_comments.divide(100).astype(int).sort_index(axis=0).sort_index(axis=1).T, 'Amount of comments (per 100)', 'Jira dataset', 'Issue type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d1ea9-5ed9-4119-8943-e923a9764ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_heatmap(df_toxic_comments.divide(100).astype(int).sort_index(axis=0).sort_index(axis=1).T, 'Amount of toxic comments (per 100)', 'Jira dataset', 'Issue type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80342f-d770-4ac7-910a-f787e06dc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in db.list_collection_names():\n",
    "    comments = get_toxic_comments(db[name], limit=1, colorize=True)\n",
    "    print(\"Comments for '\" + name + '\"')\n",
    "    for comment in comments:\n",
    "        print(comment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
